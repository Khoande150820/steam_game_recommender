{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Background\n\nFORKED FROM: \n*Wik Hung Pun*\n\nCollaborative filtering (CF) is a topic that eluded me in my quest of studying machine learning in the past. This steam games dataset gave me a reason to explore and learn more about CF. After reading some excellent works written by [Ethan Rosenthal](http://blog.ethanrosenthal.com/), [Katherine Bailey](http://katbailey.github.io/post/matrix-factorization-with-tensorflow/), and [Jesse Steinweg-Woods](https://jessesw.com/Rec-System/)","metadata":{"_uuid":"5f461b9f04abd187ab705626a60973bada715913"}},{"cell_type":"markdown","source":"# Introduction\nCollaborative filtering (CF) is a technique used for being recommender systems. The goal of CF is to infer the preferences for new items given the known preferences from all the users. [Rosenthal](http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/) has a great post explaining and implementing item- and user-based CF systems. I'd highly recommend you to read it if you are interested in recommender systems.\n\nWhat I would like to focus on is a technique calls **matrix factorization**. [There are a lot of math-y stuff behind it](https://en.wikipedia.org/wiki/Matrix_decomposition), but, for the purpose of this recommender system, we can simply think of it as solving a matrix algebra question via [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication).\n\nIn this case, if we visualize the transactions as a big matrix, where users are the rows and games are the columns. This big matrix can, in turn, be decomposed into two matrices with dimensions of *users x features* (U), and *features x games* (V). The Steinweg-Woods's image gives a clear depiction of this idea. ![id](https://jessesw.com/images/Rec_images/ALS_Image_Test.png) Once we have the U and V matrices estimated, we can then take the dot product of the two to find the predicted game preferences for each user.","metadata":{"_uuid":"9b70fff181a7a216ccfd596aa39505c5882bedb2"}},{"cell_type":"markdown","source":"## Explicit vs Implicit CF\nNow you have learned basic concept about CF, it is important to point out there are two types of CF: **explicit** and **implicit**. In explicit CF, the values we fill in the users by items matrix were preferences collected *explictly* from users (e.g., thumbs up/down, likes, user ratings, etc.). In contrast, we do not have these direct indicators of preferences from users with implicit CF. Instead, we only have *indirect* indicators such as whether they purchased the product or used the product. For instance, we only know a user bought a game in this dataset and the person might have even played it for a few hours, but we do not know if the user actually liked the game. For all we know, the user may hate the game after playing it! With implicit CF, we try to take account of these possible variations in the model.","metadata":{"_uuid":"59d2f108cb03c818b7e566b497c3144c91eaa39a"}},{"cell_type":"markdown","source":"### Miscellaneous\nPrior to actually getting to the code, it'd be remiss of me to not mention there are actually great packages available of conducting CF analysis. [Rosenthal](http://blog.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/) discussed the strengthes and weaknesses of some of these packages. Since these packages are not available on Kaggle (as fas as I know), I implemented my own here, but you should look for others' implementation if you are working outside of the Kaggle environment.","metadata":{"_uuid":"9e590c3196a5aae6efb01255fb4eb784f672d71b"}},{"cell_type":"markdown","source":"#### Code Stuff\nImport packages. Exciting stuff.","metadata":{"_uuid":"47563e3ee746f3f2d5e8bd51316c3729b2c3e0d0"}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport random\nfrom collections import Counter\nfrom sklearn.metrics import roc_curve, auc, average_precision_score","metadata":{"_uuid":"5c299b7bf892ff92ee478f37477a833bc111bcec","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n### compatability for old tf 1 code\n\ntf.compat.v1.get_default_graph()\n\ntf.compat.v1.reset_default_graph()\n\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking a look at the dataset. I named the last column as not needed since it does not appear to be associated with anything. The thing to look out for in this dataset is that the purchases and plays are separated into two separate rows. For the purpose of the analysis I will have to convert these into one record. I convert the dataset with following rules:\n1. If a game is purchased but never played, hours played = 0\n2. If a game is purchased *and* played, I keep the hours played and remove the purchase record (Playing the game implies it was purchased).","metadata":{"_uuid":"0b41e3158774f46bcc40021fd3b0c46e22dc5af9"}},{"cell_type":"code","source":"path = '../input/steam-200k.csv'\n#path = 'steam-200k.csv'\ndf = pd.read_csv(path, header = None,\n                 names = ['UserID', 'Game', 'Action', 'Hours', 'Not Needed'])\ndf.head()","metadata":{"_uuid":"43a5b0586e3039eb976c3a8517614487ed154184","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a new variable 'Hours Played' and code it as previously described.\ndf['Hours_Played'] = df['Hours'].astype('float32')\n\ndf.loc[(df['Action'] == 'purchase') & (df['Hours'] == 1.0), 'Hours_Played'] = 0","metadata":{"_uuid":"8e2ad827a6e864c561b299a3dbb2e3c7f7723cdf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort the df by User ID, games, and hours played\n# Drop the duplicated records, and unnecessary columns\ndf.UserID = df.UserID.astype('int')\ndf = df.sort_values(['UserID', 'Game', 'Hours_Played'])\n\nclean_df = df.drop_duplicates(['UserID', 'Game'], keep = 'last').drop(['Action', 'Hours', 'Not Needed'], axis = 1)\n\n# every transaction is represented by only one record now\nclean_df.head()","metadata":{"_uuid":"9a90c3f91b63a38f251b34ef2d48871dac4f7096","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_users = len(clean_df.UserID.unique())\nn_games = len(clean_df.Game.unique())\n\nprint('There are {0} users and {1} games in the data'.format(n_users, n_games))","metadata":{"_uuid":"3f757f2c2d51cf91f1bffc4e084b7679c6f63d29","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If we build a matrix of users x games, how many cells in the matrix will be filled?\nsparsity = clean_df.shape[0] / float(n_users * n_games)\nprint('{:.2%} of the user-item matrix is filled'.format(sparsity))","metadata":{"_uuid":"6b4bc348e83ad53bfba46c68ec596c0788c12a97","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here \nuser_counter = Counter()\nfor user in clean_df.UserID.tolist():\n    user_counter[user] +=1\n\ngame_counter = Counter()\nfor game in clean_df.Game.tolist():\n    game_counter[game] += 1","metadata":{"_uuid":"61e8ccec229e7bce4cb7e3f9d14471d146d6b0c2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the dictionaries to convert user and games to idx and back\nuser2idx = {user: i for i, user in enumerate(clean_df.UserID.unique())}\nidx2user = {i: user for user, i in user2idx.items()}\n\ngame2idx = {game: i for i, game in enumerate(clean_df.Game.unique())}\nidx2game = {i: game for game, i in game2idx.items()}","metadata":{"_uuid":"ac1ac04b4a6f8f80eaa0bf2feb3bc124deda2c55","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'Hours_Played' (used for confidence) is highly skewed (by players and games\n* we'll clip for stability \n* changed from orig code. also, may not be needed\n    * could try log transform also/instead","metadata":{}},{"cell_type":"code","source":"### highyl skewed distribution. maybe clip ? \n### 25% quantile is 0, 50%: 0.3 (possibly limit for returning)\n\nprint(clean_df['Hours_Played'].loc[clean_df['Hours_Played']>0.1].describe())\nprint(clean_df.loc[clean_df['Hours_Played']>0.1]['Hours_Played'].quantile(0.99), \" - 99.9 quantile\") ## 3026\n\nclean_df['Hours_Played'] = clean_df['Hours_Played'].clip(upper=500)\n\n# clean_df['Hours_Played'].hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the user and games to idx\nuser_idx = clean_df['UserID'].apply(lambda x: user2idx[x]).values\ngame_idx = clean_df['gameIdx'] = clean_df['Game'].apply(lambda x: game2idx[x]).values\nhours = clean_df['Hours_Played'].values","metadata":{"_uuid":"76d4facf2f828c4410f59f761859b118d0fbe69e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Converting the data to a user x game matrix\nSince we do not have explicit indicators in this dataset, we fill the users by games matrix with simple preferred (1) or not (0). This (preference) matrix  indicates that the user purchased and/or played the game with a 1, whereas 0 means no interaction between the user and the game. \n\nNow, one obvious quesiton is what if the users dislike their purchases? If all purchases are represented by ones, how do we find out the ones that users actually regret buying? Well, we handle this situation by constructing a **confidence matrix**. This confidence matrix has the same dimension as the preference matrix and is populated with __*hours played*__. Intuitively, it means that the more time a user spent playing the game, we have more *confidence* in that the user actually *liked/preferred* the game. ","metadata":{"_uuid":"5b8cc11619207a93878f821fe0e0c5c44bdc43a0"}},{"cell_type":"code","source":"# Using a sparse matrix will be more memory efficient and necessary for larger dataset, \n# but this works for now.\n\nzero_matrix = np.zeros(shape = (n_users, n_games)) # Create a zero matrix\nuser_game_pref = zero_matrix.copy()\nuser_game_pref[user_idx, game_idx] = 1 # Fill the matrix will preferences (bought)\n\nuser_game_interactions = zero_matrix.copy()\n# Fill the confidence with (hours played)\n# Added 1 to the hours played so that we have min. confidence for games bought but not played.\nuser_game_interactions[user_idx, game_idx] = hours + 1 ","metadata":{"_uuid":"6a50e377affdbfe0fbd89f3f0a76b4cd4d1b92b9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation\nTo examine the effectiveness of the recommender system, I used top-k precision as my evaluation metric (k = 5, in this case). In order to implement this evaluation metric, I need to first identify users who have bought more than k games and then mask k preferences from the training set. This will bias the validation process towards users with higher number of purchases. However, it makes the problem easier to handle and cold start problem of recommender system is another topic that requires a lot more in depth analysis on its own.","metadata":{"_uuid":"70cb40ff652ec92f73e9a576c291994af96d8eba"}},{"cell_type":"code","source":"k = 5\n\n# Count the number of purchases for each user\npurchase_counts = np.apply_along_axis(np.bincount, 1, user_game_pref.astype(int))\n# buyers_idx = np.where(purchase_counts[:, 1] >= 2 * k)[0] #find the users who purchase 2 * k games\n# print('{0} users bought {1} or more games'.format(len(buyers_idx), 2 * k))\n\nbuyers_idx = np.where(purchase_counts[:, 1] >= int(1.6 * k))[0] #find the users who purchase 2 * k games\nprint('{0} users bought {1} or more games'.format(len(buyers_idx), int(1.6 * k)))","metadata":{"_uuid":"54aaf7d93dc19be314657bdb0bf87ac79e5acfed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_frac = 0.2 # Let's save 10% of the data for validation and 10% for testing.\ntest_users_idx = np.random.choice(buyers_idx,\n                                  size = int(np.ceil(len(buyers_idx) * test_frac)),\n                                  replace = False)","metadata":{"_uuid":"fddf039e653a59692e0977b7d84278c85e2686bd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_users_idx = test_users_idx[:int(len(test_users_idx) / 2)]\ntest_users_idx = test_users_idx[int(len(test_users_idx) / 2):]","metadata":{"_uuid":"9710841cddf65ea950c974292b2deaae0ff0644b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A function used to mask the preferences data from training matrix\ndef data_process(dat, train, test, user_idx, k):\n    for user in user_idx:\n        purchases = np.where(dat[user, :] == 1)[0]\n        mask = np.random.choice(purchases, size = k, replace = False)\n        \n        train[user, mask] = 0\n        test[user, mask] = dat[user, mask]\n    return train, test","metadata":{"_uuid":"534397813aef17085c2731a0176820d19c725bad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_matrix = user_game_pref.copy()\ntest_matrix = zero_matrix.copy()\nval_matrix = zero_matrix.copy()\n\n# Mask the train matrix and create the validation and test matrices\ntrain_matrix, val_matrix = data_process(user_game_pref, train_matrix, val_matrix, val_users_idx, k)\ntrain_matrix, test_matrix = data_process(user_game_pref, train_matrix, test_matrix, test_users_idx, k)","metadata":{"_uuid":"786e22be3423b9d1aaf26d6438c7013740692090","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's take a look at what was actually accomplised\n# You can see the test matrix preferences are masked in the train matrix\ntest_matrix[test_users_idx[0], test_matrix[test_users_idx[0], :].nonzero()[0]]","metadata":{"_uuid":"237203d225369a9c781b66993470500aff764891","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_matrix[test_users_idx[0], test_matrix[test_users_idx[0], :].nonzero()[0]]","metadata":{"_uuid":"c7a1cb328120060a6ec633af1112ec857792724a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tensorflow Implementation\nI implemented the implicit CF in tensorflow because I want to get more familiar with it. You can do this with only scipy and numpy. In tensorflow, you have to define the computation graph first and then actually carry out the calculations.","metadata":{"_uuid":"eea97f60b9e5f7f983f7de588ab29559b4debdf8"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tf.reset_default_graph() # Create a new graphs\n\npref = tf.placeholder(tf.float32, (n_users, n_games))  # Here's the preference matrix\ninteractions = tf.placeholder(tf.float32, (n_users, n_games)) # Here's the hours played matrix\nusers_idx = tf.placeholder(tf.int32, (None))","metadata":{"_uuid":"e021b17a064b41854fe21804fe3f30076cea5aa8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of directly multiplying the hours played matrix with the preference matrix, we want to add a confidence parameter. We can think of it as how much weight we should give to these interactions. The [original paper](http://yifanhu.net/PUB/cf.pdf) recommends setting the parameter to 40, but I found the result to be less than ideal (was 30). Instead, I sampled it from a uniform distribution and use gradient descent to find the optimal value.","metadata":{"_uuid":"597acc8b735fecc04cae3b7b1e7e6456562bbe94"}},{"cell_type":"code","source":"n_features = 20 # Number of latent features to be extracted , orig: 30\n\n# The X matrix represents the user latent preferences with a shape of user x latent features\nX = tf.Variable(tf.truncated_normal([n_users, n_features], mean = 0, stddev = 0.05))\n\n# The Y matrix represents the game latent features with a shape of game x latent features\nY = tf.Variable(tf.truncated_normal([n_games, n_features], mean = 0, stddev = 0.05))\n\n# Here's the initilization of the confidence parameter\nconf_alpha = tf.Variable(tf.random_uniform([1], 0, 1))","metadata":{"_uuid":"402e2bee4305c6956cbc3d579d1f0a27ed5023ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## User and Item Bias\nTwo other model parameters that I did not mention above are user and item biases. Intuitively, we would expect some users play games at a faster pace than the others. Similarly, we also expect some games would take less time to complete. User and item biases allow us to express these intuitions in a statistical model where we expect there are systematic differences in how users interact with the games and how games are played.\n\nOne thing to note is that both biases are not necessary to build the recommender system, however, I found including these parameters improved the recommender system.","metadata":{"_uuid":"2b727b5f7282b6081bd92af24bfefe19a9bf256b"}},{"cell_type":"code","source":"# Initialize a user bias vector\nuser_bias = tf.Variable(tf.truncated_normal([n_users, 1], stddev = 0.2))\n\n# Concatenate the vector to the user matrix\n# Due to how matrix algebra works, we also need to add a column of ones to make sure\n# the resulting calculation will take into account the item biases.\nX_plus_bias = tf.concat([X, \n                         #tf.convert_to_tensor(user_bias, dtype = tf.float32),\n                         user_bias,\n                         tf.ones((n_users, 1), dtype = tf.float32)], axis = 1)","metadata":{"_uuid":"4b497db519864c9f64766733813417ff330113a9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the item bias vector\nitem_bias = tf.Variable(tf.truncated_normal([n_games, 1], stddev = 0.2))\n\n# Cocatenate the vector to the game matrix\n# Also, adds a column one for the same reason stated above.\nY_plus_bias = tf.concat([Y, \n                         tf.ones((n_games, 1), dtype = tf.float32),\n                         item_bias],\n                         axis = 1)","metadata":{"_uuid":"e3ac06b75c580a4dfda6aa7d9732566e5dc89e0f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here, we finally multiply the matrices together to estimate the predicted preferences\npred_pref = tf.matmul(X_plus_bias, Y_plus_bias, transpose_b=True)\n\n# Construct the confidence matrix with the hours played and alpha paramter\nconf = 1 + conf_alpha * interactions","metadata":{"_uuid":"f338fa54d4d79dedaf06c81e03a83107df285182","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cost of the model would be the squared sum of predicted preferences and actual preferences. This cost is modified by the conference matrix. Finally, l2-regularization is also added to avoid overfitting the training set.","metadata":{"_uuid":"b1bbb31d2b71a6eeffa8c21536d89755256f8cd0"}},{"cell_type":"code","source":"cost = tf.reduce_sum(tf.multiply(conf, tf.square(tf.subtract(pref, pred_pref))))\nl2_sqr = tf.nn.l2_loss(X) + tf.nn.l2_loss(Y) + tf.nn.l2_loss(user_bias) + tf.nn.l2_loss(item_bias)\nlambda_c = 0.01\nloss = cost + lambda_c * l2_sqr","metadata":{"_uuid":"211103a111b2dc40cf3193ffd7391de26447632d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 0.04\noptimize = tf.train.AdagradOptimizer(learning_rate = lr).minimize(loss)","metadata":{"_uuid":"6837caa862d8189139826a2959ac058c1f7e5103","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is a function that helps to calculate the top k precision \ndef top_k_precision(pred, mat, k, user_idx):\n    precisions = []\n    \n    for user in user_idx:\n        rec = np.argsort(-pred[user, :]) # Found the top recommendation from the predictions\n        \n        top_k = rec[:k]\n        labels = mat[user, :].nonzero()[0]\n        \n        precision = len(set(top_k) & set(labels)) / float(k) # Calculate the precisions from actual labels\n        precisions.append(precision)\n    return np.mean(precisions) ","metadata":{"_uuid":"bad165c94772a2d907116e60aaaa4364b197e894","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's the actual training. I calculate the validation precision after every 10 iterations. With higher number of iteration seems to overfit so I settled with 80.","metadata":{"_uuid":"74e97b2d80a4b7336ee7ac7f037c230dd13f6616"}},{"cell_type":"code","source":"iterations = 80\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    \n    for i in range(iterations):\n        sess.run(optimize, feed_dict = {pref: train_matrix,\n                                        interactions: user_game_interactions})\n        \n        if i % 10 == 0:\n            mod_loss = sess.run(loss, feed_dict = {pref: train_matrix,\n                                                   interactions: user_game_interactions})            \n            mod_pred = pred_pref.eval()\n            train_precision = top_k_precision(mod_pred, train_matrix, k, val_users_idx)\n            val_precision = top_k_precision(mod_pred, val_matrix, k, val_users_idx)\n            print('Iterations {0}...'.format(i),\n                  'Training Loss {:.2f}...'.format(mod_loss),\n                  'Train Precision {:.3f}...'.format(train_precision),\n                  'Val Precision {:.3f}'.format(val_precision)\n                )\n\n    rec = pred_pref.eval()\n    test_precision = top_k_precision(rec, test_matrix, k, test_users_idx)\n    print('\\n')\n    print('Test Precision{:.3f}'.format(test_precision))","metadata":{"_uuid":"351bff2fefb97e61f4919dc142694b53383d8f42","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test top-k precision is not that high but I did not spend a lot of time optimizing the hyperparameters so there is probably a lot more room for improvement. Something for you to try if you are interested.","metadata":{"_uuid":"fb02464250f9c2804051c5b29b626586a8f34b9e"}},{"cell_type":"markdown","source":"## Examples\nBelow I print out a few examples of recommendations accompanied with the actual purchases of the users. Some recommendations made more sense than the others but overall precision is fairly low.","metadata":{"_uuid":"2191459c3f7f182d144b14b5f66c6730f1990c2d"}},{"cell_type":"code","source":"n_examples = 9\nusers = np.random.choice(test_users_idx, size = n_examples, replace = False)\nrec_games = np.argsort(-rec)","metadata":{"_uuid":"94eae1c87b58395226882fdb5e5048366088092d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for user in users:\n    print('Recommended Games for {0} are ...'.format(idx2user[user]))\n    purchase_history = np.where(train_matrix[user, :] != 0)[0]\n    recommendations = rec_games[user, :]\n\n    \n    new_recommendations = recommendations[~np.in1d(recommendations, purchase_history)][:k]\n    \n    print('We recommend these games')\n    print(', '.join([idx2game[game] for game in new_recommendations]))\n    print('\\n')\n    print('The games that the user actually purchased are ...')\n    print(', '.join([idx2game[game] for game in np.where(test_matrix[user, :] != 0)[0]]))\n    print('\\n')\n    print('Precision of {0}'.format(len(set(new_recommendations) & set(np.where(test_matrix[user, :] != 0)[0])) / float(k)))\n    print('--------------------------------------')\n    print('\\n')","metadata":{"_uuid":"b948ea4c386532571ef352accecc3f9899d18850","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Words\nBeside optimizing the hyperparameters, I did not spend a lot of time cleaning the data. Some of the games are more like DLCs and variations of the game. Consolidating some of these games may yield better results. \n\nThere are  packages out there that can build the implicit CF for you ","metadata":{"_uuid":"2aa4b311370d127e745bf1e21aad822c827ef029"}}]}